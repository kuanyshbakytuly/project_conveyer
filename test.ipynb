{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied video.mov to Videos/video_1.mov\n",
      "Copied video.mov to Videos/video_2.mov\n",
      "Copied video.mov to Videos/video_3.mov\n",
      "Copied video.mov to Videos/video_4.mov\n",
      "Copied video.mov to Videos/video_5.mov\n",
      "Copied video.mov to Videos/video_6.mov\n",
      "Copied video.mov to Videos/video_7.mov\n",
      "Copied video.mov to Videos/video_8.mov\n",
      "Copied video.mov to Videos/video_9.mov\n",
      "Copied video.mov to Videos/video_10.mov\n",
      "Copied video.mov to Videos/video_11.mov\n",
      "Copied video.mov to Videos/video_12.mov\n",
      "Copied video.mov to Videos/video_13.mov\n",
      "Copied video.mov to Videos/video_14.mov\n",
      "Copied video.mov to Videos/video_15.mov\n",
      "Copied video.mov to Videos/video_16.mov\n",
      "Copied video.mov to Videos/video_17.mov\n",
      "Copied video.mov to Videos/video_18.mov\n",
      "Copied video.mov to Videos/video_19.mov\n",
      "Copied video.mov to Videos/video_20.mov\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define source file and destination directory\n",
    "source_file = 'video.mov'  # Path to the source file you want to copy\n",
    "destination_folder = 'Videos'  # Folder where you want to copy the files\n",
    "\n",
    "# Ensure the destination folder exists, if not, create it\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "\n",
    "# Copy and rename the file for video_1.mov to video_20.mov\n",
    "for i in range(1, 21):  # From 1 to 20\n",
    "    # Define the new filename\n",
    "    new_filename = f\"video_{i}.mov\"\n",
    "    \n",
    "    # Define the full path for the new file\n",
    "    destination_file = os.path.join(destination_folder, new_filename)\n",
    "    \n",
    "    # Copy the file to the new location with the new name\n",
    "    shutil.copy(source_file, destination_file)\n",
    "    \n",
    "    print(f\"Copied {source_file} to {destination_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 12 18:42:37 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 575.51.02              Driver Version: 575.51.02      CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:01:00.0 Off |                  Off |\n",
      "|  0%   44C    P8             37W /  360W |      17MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        On  |   00000000:41:00.0 Off |                  Off |\n",
      "|  0%   42C    P8             36W /  360W |      17MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cv_api.src.detection.detection import PotatoDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_area = [432, 0, 779, 720]  # [x1, y1, x2, y2]\n",
    "cm_per_pixel = 0.02997\n",
    "conf = 0.6\n",
    "tracker_config = 'bytetrack_custom.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection = PotatoDetector(\n",
    "        model_path='Models/model_det_fp16_1/best.engine',\n",
    "        camera_id=1,\n",
    "        task='detect',\n",
    "        tracker_config=tracker_config,\n",
    "        segmentation_area=segmentation_area,\n",
    "        track_ids=set(),\n",
    "        device=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[[133, 139, 119],\n",
       "          [133, 139, 121],\n",
       "          [132, 138, 120],\n",
       "          ...,\n",
       "          [144, 157, 128],\n",
       "          [144, 158, 126],\n",
       "          [144, 158, 126]],\n",
       "  \n",
       "         [[135, 141, 121],\n",
       "          [137, 142, 125],\n",
       "          [135, 140, 123],\n",
       "          ...,\n",
       "          [145, 160, 126],\n",
       "          [145, 161, 124],\n",
       "          [145, 161, 124]],\n",
       "  \n",
       "         [[138, 144, 124],\n",
       "          [138, 144, 125],\n",
       "          [137, 143, 124],\n",
       "          ...,\n",
       "          [146, 162, 124],\n",
       "          [146, 163, 122],\n",
       "          [146, 163, 122]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[ 79, 169, 242],\n",
       "          [ 78, 168, 241],\n",
       "          [ 77, 167, 240],\n",
       "          ...,\n",
       "          [ 75, 119, 152],\n",
       "          [ 73, 126, 172],\n",
       "          [ 83, 136, 182]],\n",
       "  \n",
       "         [[ 80, 169, 242],\n",
       "          [ 79, 168, 241],\n",
       "          [ 78, 167, 240],\n",
       "          ...,\n",
       "          [ 75, 115, 145],\n",
       "          [ 74, 122, 164],\n",
       "          [ 83, 131, 173]],\n",
       "  \n",
       "         [[ 80, 169, 242],\n",
       "          [ 79, 167, 242],\n",
       "          [ 78, 166, 241],\n",
       "          ...,\n",
       "          [ 74, 108, 137],\n",
       "          [ 73, 115, 156],\n",
       "          [ 84, 126, 167]]], dtype=uint8),\n",
       "  array([[[ 80, 104,  80],\n",
       "          [ 84, 105,  71],\n",
       "          [ 83, 104,  70],\n",
       "          ...,\n",
       "          [ 75, 149, 212],\n",
       "          [ 72, 146, 209],\n",
       "          [ 66, 141, 201]],\n",
       "  \n",
       "         [[ 72, 100,  78],\n",
       "          [ 82, 102,  68],\n",
       "          [ 81, 101,  67],\n",
       "          ...,\n",
       "          [ 71, 148, 212],\n",
       "          [ 68, 145, 209],\n",
       "          [ 61, 139, 200]],\n",
       "  \n",
       "         [[ 69,  98,  81],\n",
       "          [ 80, 100,  69],\n",
       "          [ 79,  99,  68],\n",
       "          ...,\n",
       "          [ 70, 148, 213],\n",
       "          [ 66, 144, 209],\n",
       "          [ 59, 137, 200]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[116, 117,  73],\n",
       "          [116, 116,  74],\n",
       "          [116, 116,  74],\n",
       "          ...,\n",
       "          [106, 184, 247],\n",
       "          [105, 183, 246],\n",
       "          [108, 183, 246]],\n",
       "  \n",
       "         [[118, 119,  73],\n",
       "          [118, 119,  75],\n",
       "          [118, 119,  75],\n",
       "          ...,\n",
       "          [106, 184, 247],\n",
       "          [105, 183, 246],\n",
       "          [108, 183, 246]],\n",
       "  \n",
       "         [[119, 120,  74],\n",
       "          [119, 120,  76],\n",
       "          [120, 121,  77],\n",
       "          ...,\n",
       "          [105, 183, 246],\n",
       "          [105, 183, 246],\n",
       "          [108, 183, 246]]], dtype=uint8),\n",
       "  array([[[135, 148, 116],\n",
       "          [134, 148, 114],\n",
       "          [135, 149, 115],\n",
       "          ...,\n",
       "          [ 92, 153, 221],\n",
       "          [ 91, 152, 222],\n",
       "          [ 90, 151, 221]],\n",
       "  \n",
       "         [[134, 146, 116],\n",
       "          [135, 148, 115],\n",
       "          [136, 149, 116],\n",
       "          ...,\n",
       "          [ 89, 150, 218],\n",
       "          [ 88, 149, 218],\n",
       "          [ 89, 150, 219]],\n",
       "  \n",
       "         [[134, 146, 117],\n",
       "          [134, 147, 116],\n",
       "          [135, 148, 117],\n",
       "          ...,\n",
       "          [ 84, 146, 214],\n",
       "          [ 85, 146, 215],\n",
       "          [ 86, 147, 216]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[137, 162, 102],\n",
       "          [137, 162, 102],\n",
       "          [137, 162, 102],\n",
       "          ...,\n",
       "          [ 89, 108, 101],\n",
       "          [ 82, 114, 136],\n",
       "          [ 91, 123, 145]],\n",
       "  \n",
       "         [[136, 161, 102],\n",
       "          [136, 161, 102],\n",
       "          [136, 161, 102],\n",
       "          ...,\n",
       "          [ 87, 103,  90],\n",
       "          [ 80, 109, 126],\n",
       "          [ 90, 119, 136]],\n",
       "  \n",
       "         [[135, 159, 102],\n",
       "          [135, 159, 102],\n",
       "          [135, 159, 102],\n",
       "          ...,\n",
       "          [ 88, 100,  81],\n",
       "          [ 81, 104, 116],\n",
       "          [ 90, 113, 125]]], dtype=uint8)],\n",
       " [[579, 335, 718, 461, 3], [603, 461, 731, 601, 9], [443, 427, 614, 568, 12]],\n",
       " 0.018310070037841797,\n",
       " 0.0017933845520019531,\n",
       " 0.0008864402770996094)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection.track(frame.copy(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpegcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('Models/model_det_test/best.engine', task='detect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = ffmpegcv.VideoCaptureNV('video.mov', pix_fmt='bgr24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, frame = cap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Models/model_det_test/best.engine for TensorRT inference...\n",
      "[04/24/2025-01:04:23] [TRT] [I] Loaded engine size: 9 MiB\n",
      "[04/24/2025-01:04:23] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +181, now: CPU 0, GPU 186 (MiB)\n",
      "\n",
      "0: 384x640 17 potatos, 2.1ms\n",
      "Speed: 19.2ms preprocess, 2.1ms inference, 153.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'potato'}\n",
       " obb: None\n",
       " orig_img: array([[[ 97, 123,  52],\n",
       "         [ 97, 123,  52],\n",
       "         [ 97, 123,  52],\n",
       "         ...,\n",
       "         [122, 143,  93],\n",
       "         [122, 143,  93],\n",
       "         [122, 143,  93]],\n",
       " \n",
       "        [[ 96, 124,  52],\n",
       "         [ 96, 124,  52],\n",
       "         [ 96, 124,  52],\n",
       "         ...,\n",
       "         [121, 144,  93],\n",
       "         [121, 144,  93],\n",
       "         [121, 144,  93]],\n",
       " \n",
       "        [[ 97, 123,  51],\n",
       "         [ 97, 123,  51],\n",
       "         [ 97, 123,  51],\n",
       "         ...,\n",
       "         [122, 143,  93],\n",
       "         [122, 143,  93],\n",
       "         [122, 143,  93]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[118, 138,  66],\n",
       "         [120, 140,  68],\n",
       "         [120, 140,  68],\n",
       "         ...,\n",
       "         [135, 163,  95],\n",
       "         [135, 163,  95],\n",
       "         [135, 163,  95]],\n",
       " \n",
       "        [[116, 137,  65],\n",
       "         [117, 138,  66],\n",
       "         [117, 138,  66],\n",
       "         ...,\n",
       "         [132, 161,  94],\n",
       "         [132, 161,  94],\n",
       "         [132, 161,  94]],\n",
       " \n",
       "        [[116, 137,  65],\n",
       "         [117, 138,  66],\n",
       "         [117, 138,  66],\n",
       "         ...,\n",
       "         [132, 161,  94],\n",
       "         [132, 161,  94],\n",
       "         [132, 161,  94]]], dtype=uint8)\n",
       " orig_shape: (1080, 1920)\n",
       " path: 'image0.jpg'\n",
       " probs: None\n",
       " save_dir: '/home/paperspace/project_conveyer/runs/detect/track'\n",
       " speed: {'preprocess': 19.198250999579614, 'inference': 2.0897930003229703, 'postprocess': 153.66866700014725}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.track(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.9.0.34'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorrt.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è TensorRT requires GPU export, automatically assigning device=0\n",
      "Ultralytics 8.3.114 üöÄ Python-3.10.12 torch-2.8.0.dev20250408+cu128 CUDA:0 (NVIDIA RTX A6000, 48550MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'Models/model_det_test/best.pt' with input shape (20, 3, 640, 640) BCHW and output shape(s) (20, 5, 8400) (6.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.50...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 11.6s, saved as 'Models/model_det_test/best.onnx' (11.6 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.9.0.34...\n",
      "[04/24/2025-01:31:16] [TRT] [I] The logger passed into createInferBuilder differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "[04/24/2025-01:31:16] [TRT] [I] ----------------------------------------------------------------\n",
      "[04/24/2025-01:31:16] [TRT] [I] Input filename:   Models/model_det_test/best.onnx\n",
      "[04/24/2025-01:31:16] [TRT] [I] ONNX IR version:  0.0.9\n",
      "[04/24/2025-01:31:16] [TRT] [I] Opset version:    19\n",
      "[04/24/2025-01:31:16] [TRT] [I] Producer name:    pytorch\n",
      "[04/24/2025-01:31:16] [TRT] [I] Producer version: 2.8.0\n",
      "[04/24/2025-01:31:16] [TRT] [I] Domain:           \n",
      "[04/24/2025-01:31:16] [TRT] [I] Model version:    0\n",
      "[04/24/2025-01:31:16] [TRT] [I] Doc string:       \n",
      "[04/24/2025-01:31:16] [TRT] [I] ----------------------------------------------------------------\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(-1, 3, -1, -1) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(-1, 5, -1) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as Models/model_det_test/best.engine\n",
      "[04/24/2025-01:31:16] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[04/24/2025-01:35:17] [TRT] [I] Detected 1 inputs and 3 output network tensors.\n",
      "[04/24/2025-01:35:19] [TRT] [I] Total Host Persistent Memory: 384560 bytes\n",
      "[04/24/2025-01:35:19] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[04/24/2025-01:35:19] [TRT] [I] Max Scratch Memory: 0 bytes\n",
      "[04/24/2025-01:35:19] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 145 steps to complete.\n",
      "[04/24/2025-01:35:19] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 8.77636ms to assign 10 blocks to 145 nodes requiring 189606400 bytes.\n",
      "[04/24/2025-01:35:19] [TRT] [I] Total Activation Memory: 189605376 bytes\n",
      "[04/24/2025-01:35:19] [TRT] [I] Total Weights Memory: 6040072 bytes\n",
      "[04/24/2025-01:35:19] [TRT] [I] Engine generation completed in 243.192 seconds.\n",
      "[04/24/2025-01:35:19] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 500 MiB\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ‚úÖ 254.9s, saved as 'Models/model_det_test/best.engine' (9.6 MB)\n",
      "\n",
      "Export complete (255.1s)\n",
      "Results saved to \u001b[1m/home/paperspace/project_conveyer/Models/model_det_test\u001b[0m\n",
      "Predict:         yolo predict task=detect model=Models/model_det_test/best.engine imgsz=640 half \n",
      "Validate:        yolo val task=detect model=Models/model_det_test/best.engine imgsz=640 data=C:\\Users\\Asus Rog Strix\\Desktop\\Multimodal\\potato check.v4i.yolov8\\data.yaml half \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Models/model_det_test/best.engine'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"Models/model_det_test/best.pt\")\n",
    "model.export(\n",
    "    format=\"engine\",\n",
    "    dynamic=True,  \n",
    "    imgsz=(640, 640),\n",
    "    batch=20,  \n",
    "    half=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"Models/model_seg_fp32/best_yoloseg.pt\")\n",
    "\n",
    "engine_file = model.export(format=\"engine\", simplify=True, dynamic=True, imgsz=(320, 320), half=True, batch=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp -r /home/paperspace/project_conveyer/Datasets /home/paperspace/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpegcv\n",
    "from ultralytics import YOLO\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import psutil\n",
    "from pynvml import *\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('Models/model_det_int8/best.engine', task='detect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLO()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.25M/6.25M [00:00<00:00, 331MB/s]\n"
     ]
    }
   ],
   "source": [
    "model1 = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_check_is_pytorch_model',\n",
       " '_compiled_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_new',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_reset_ckpt_args',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_smart_load',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_callback',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'benchmark',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'callbacks',\n",
       " 'cfg',\n",
       " 'children',\n",
       " 'ckpt',\n",
       " 'ckpt_path',\n",
       " 'clear_callback',\n",
       " 'compile',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'device',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'embed',\n",
       " 'eval',\n",
       " 'export',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'fuse',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'info',\n",
       " 'ipu',\n",
       " 'is_hub_model',\n",
       " 'is_triton_model',\n",
       " 'load',\n",
       " 'load_state_dict',\n",
       " 'metrics',\n",
       " 'model',\n",
       " 'model_name',\n",
       " 'modules',\n",
       " 'mtia',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'names',\n",
       " 'overrides',\n",
       " 'parameters',\n",
       " 'predict',\n",
       " 'predictor',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_load_state_dict_pre_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_post_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'reset_callbacks',\n",
       " 'reset_weights',\n",
       " 'save',\n",
       " 'session',\n",
       " 'set_extra_state',\n",
       " 'set_submodule',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'task',\n",
       " 'task_map',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'track',\n",
       " 'train',\n",
       " 'trainer',\n",
       " 'transforms',\n",
       " 'tune',\n",
       " 'type',\n",
       " 'val',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpegcv\n",
    "cap = ffmpegcv.VideoCaptureNV('video.mp4', pix_fmt='bgr24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, orig_frame = cap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 17 potatos, 1.3ms\n",
      "Speed: 2.2ms preprocess, 1.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(orig_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = [0].boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2794,  666, 3063,  920],\n",
       "       [1416,  776, 1693, 1050],\n",
       "       [1529, 1402, 1827, 1669],\n",
       "       [1156,  670, 1434,  925],\n",
       "       [1747, 1168, 2076, 1490],\n",
       "       [1805, 1506, 2130, 1878],\n",
       "       [1393, 1037, 1723, 1366],\n",
       "       [1205,  921, 1462, 1204],\n",
       "       [2134, 1493, 2387, 1785],\n",
       "       [ 885,  854, 1228, 1137],\n",
       "       [3036,  181, 3390,  488],\n",
       "       [   0,  670,  289, 1031],\n",
       "       [3543, 1474, 3840, 1757],\n",
       "       [3281, 1524, 3639, 1794],\n",
       "       [ 114,  408,  501,  720],\n",
       "       [3636, 1108, 3840, 1408],\n",
       "       [3659,  476, 3840,  855]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "cls: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
       "conf: tensor([0.9328, 0.9322, 0.9268, 0.9227, 0.9219, 0.9182, 0.9176, 0.9157, 0.9113, 0.9069, 0.9063, 0.8992, 0.8980, 0.8969, 0.8860, 0.8167, 0.8031], device='cuda:0')\n",
       "data: tensor([[2.7941e+03, 6.6614e+02, 3.0634e+03, 9.2082e+02, 9.3275e-01, 0.0000e+00],\n",
       "        [1.4165e+03, 7.7637e+02, 1.6933e+03, 1.0508e+03, 9.3224e-01, 0.0000e+00],\n",
       "        [1.5294e+03, 1.4026e+03, 1.8278e+03, 1.6693e+03, 9.2684e-01, 0.0000e+00],\n",
       "        [1.1563e+03, 6.7065e+02, 1.4344e+03, 9.2583e+02, 9.2268e-01, 0.0000e+00],\n",
       "        [1.7475e+03, 1.1690e+03, 2.0764e+03, 1.4907e+03, 9.2193e-01, 0.0000e+00],\n",
       "        [1.8057e+03, 1.5061e+03, 2.1303e+03, 1.8785e+03, 9.1818e-01, 0.0000e+00],\n",
       "        [1.3940e+03, 1.0373e+03, 1.7233e+03, 1.3665e+03, 9.1764e-01, 0.0000e+00],\n",
       "        [1.2058e+03, 9.2148e+02, 1.4623e+03, 1.2043e+03, 9.1566e-01, 0.0000e+00],\n",
       "        [2.1342e+03, 1.4939e+03, 2.3877e+03, 1.7850e+03, 9.1129e-01, 0.0000e+00],\n",
       "        [8.8582e+02, 8.5487e+02, 1.2284e+03, 1.1377e+03, 9.0694e-01, 0.0000e+00],\n",
       "        [3.0369e+03, 1.8163e+02, 3.3908e+03, 4.8820e+02, 9.0633e-01, 0.0000e+00],\n",
       "        [0.0000e+00, 6.7019e+02, 2.8976e+02, 1.0314e+03, 8.9922e-01, 0.0000e+00],\n",
       "        [3.5434e+03, 1.4748e+03, 3.8400e+03, 1.7571e+03, 8.9797e-01, 0.0000e+00],\n",
       "        [3.2812e+03, 1.5246e+03, 3.6397e+03, 1.7941e+03, 8.9693e-01, 0.0000e+00],\n",
       "        [1.1494e+02, 4.0870e+02, 5.0186e+02, 7.2030e+02, 8.8599e-01, 0.0000e+00],\n",
       "        [3.6363e+03, 1.1087e+03, 3.8400e+03, 1.4083e+03, 8.1673e-01, 0.0000e+00],\n",
       "        [3.6596e+03, 4.7672e+02, 3.8400e+03, 8.5577e+02, 8.0315e-01, 0.0000e+00]], device='cuda:0')\n",
       "id: None\n",
       "is_track: False\n",
       "orig_shape: (2160, 3840)\n",
       "shape: torch.Size([17, 6])\n",
       "xywh: tensor([[2928.7510,  793.4778,  269.2937,  254.6797],\n",
       "        [1554.9171,  913.6091,  276.7742,  274.4810],\n",
       "        [1678.6326, 1535.9681,  298.4054,  266.7385],\n",
       "        [1295.3325,  798.2377,  278.0698,  255.1776],\n",
       "        [1911.9565, 1329.8258,  328.9761,  321.6833],\n",
       "        [1968.0007, 1692.3079,  324.6914,  372.3706],\n",
       "        [1558.6443, 1201.9125,  329.3696,  329.1702],\n",
       "        [1334.0874, 1062.9077,  256.4888,  282.8629],\n",
       "        [2260.9521, 1639.4725,  253.4456,  291.0930],\n",
       "        [1057.1323,  996.2974,  342.6236,  282.8452],\n",
       "        [3213.8413,  334.9149,  353.9619,  306.5649],\n",
       "        [ 144.8796,  850.7771,  289.7592,  361.1753],\n",
       "        [3691.6807, 1615.9465,  296.6389,  282.3014],\n",
       "        [3460.4553, 1659.3373,  358.4263,  269.5535],\n",
       "        [ 308.3967,  564.5024,  386.9219,  311.5961],\n",
       "        [3738.1621, 1258.5134,  203.6755,  299.6666],\n",
       "        [3749.7798,  666.2440,  180.4402,  379.0490]], device='cuda:0')\n",
       "xywhn: tensor([[0.7627, 0.3674, 0.0701, 0.1179],\n",
       "        [0.4049, 0.4230, 0.0721, 0.1271],\n",
       "        [0.4371, 0.7111, 0.0777, 0.1235],\n",
       "        [0.3373, 0.3696, 0.0724, 0.1181],\n",
       "        [0.4979, 0.6157, 0.0857, 0.1489],\n",
       "        [0.5125, 0.7835, 0.0846, 0.1724],\n",
       "        [0.4059, 0.5564, 0.0858, 0.1524],\n",
       "        [0.3474, 0.4921, 0.0668, 0.1310],\n",
       "        [0.5888, 0.7590, 0.0660, 0.1348],\n",
       "        [0.2753, 0.4612, 0.0892, 0.1309],\n",
       "        [0.8369, 0.1551, 0.0922, 0.1419],\n",
       "        [0.0377, 0.3939, 0.0755, 0.1672],\n",
       "        [0.9614, 0.7481, 0.0772, 0.1307],\n",
       "        [0.9012, 0.7682, 0.0933, 0.1248],\n",
       "        [0.0803, 0.2613, 0.1008, 0.1443],\n",
       "        [0.9735, 0.5826, 0.0530, 0.1387],\n",
       "        [0.9765, 0.3084, 0.0470, 0.1755]], device='cuda:0')\n",
       "xyxy: tensor([[2794.1040,  666.1379, 3063.3977,  920.8176],\n",
       "        [1416.5300,  776.3687, 1693.3042, 1050.8496],\n",
       "        [1529.4299, 1402.5989, 1827.8353, 1669.3374],\n",
       "        [1156.2976,  670.6489, 1434.3674,  925.8265],\n",
       "        [1747.4685, 1168.9841, 2076.4446, 1490.6675],\n",
       "        [1805.6550, 1506.1226, 2130.3464, 1878.4932],\n",
       "        [1393.9595, 1037.3274, 1723.3291, 1366.4976],\n",
       "        [1205.8430,  921.4763, 1462.3318, 1204.3391],\n",
       "        [2134.2295, 1493.9260, 2387.6750, 1785.0190],\n",
       "        [ 885.8205,  854.8748, 1228.4441, 1137.7200],\n",
       "        [3036.8604,  181.6325, 3390.8223,  488.1974],\n",
       "        [   0.0000,  670.1895,  289.7592, 1031.3647],\n",
       "        [3543.3611, 1474.7959, 3840.0000, 1757.0973],\n",
       "        [3281.2422, 1524.5605, 3639.6685, 1794.1140],\n",
       "        [ 114.9358,  408.7044,  501.8577,  720.3005],\n",
       "        [3636.3245, 1108.6801, 3840.0000, 1408.3467],\n",
       "        [3659.5598,  476.7195, 3840.0000,  855.7684]], device='cuda:0')\n",
       "xyxyn: tensor([[0.7276, 0.3084, 0.7978, 0.4263],\n",
       "        [0.3689, 0.3594, 0.4410, 0.4865],\n",
       "        [0.3983, 0.6494, 0.4760, 0.7728],\n",
       "        [0.3011, 0.3105, 0.3735, 0.4286],\n",
       "        [0.4551, 0.5412, 0.5407, 0.6901],\n",
       "        [0.4702, 0.6973, 0.5548, 0.8697],\n",
       "        [0.3630, 0.4802, 0.4488, 0.6326],\n",
       "        [0.3140, 0.4266, 0.3808, 0.5576],\n",
       "        [0.5558, 0.6916, 0.6218, 0.8264],\n",
       "        [0.2307, 0.3958, 0.3199, 0.5267],\n",
       "        [0.7908, 0.0841, 0.8830, 0.2260],\n",
       "        [0.0000, 0.3103, 0.0755, 0.4775],\n",
       "        [0.9228, 0.6828, 1.0000, 0.8135],\n",
       "        [0.8545, 0.7058, 0.9478, 0.8306],\n",
       "        [0.0299, 0.1892, 0.1307, 0.3335],\n",
       "        [0.9470, 0.5133, 1.0000, 0.6520],\n",
       "        [0.9530, 0.2207, 1.0000, 0.3962]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2160, 3840, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "orig_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pycuda==2025.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_frame.shape\n",
    "torch.Size([3, 2160, 3840])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —É —Ç–µ–±—è –µ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ orig_frame —Å shape [3, 2160, 3840]\n",
    "# –î–æ–±–∞–≤–∏–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞\n",
    "input_tensor = orig_frame.unsqueeze(0)  # -> [1, 3, 2160, 3840]\n",
    "\n",
    "# –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ —Ç–µ–Ω–∑–æ—Ä –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —Ç–æ–º –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ, —á—Ç–æ –∏ –º–æ–¥–µ–ª—å\n",
    "input_tensor = input_tensor.float() / 255.0  # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏\n",
    "with torch.no_grad():\n",
    "    results = model(input_tensor, imgsz=640)\n",
    "\n",
    "# –ù–∞–ø—Ä–∏–º–µ—Ä, –≤—ã–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0].boxes[0].xyxy[0].cpu().numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = ffmpegcv.toCUDA(ffmpegcv.VideoCaptureNV('video.mp4', pix_fmt='nv12'), tensor_format='chw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = orig_frame.permute(1, 2, 0).cpu().numpy()\n",
    "cv2_image = cv2.cvtColor(inp, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = cv2.imread('test1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(n, conf=0.5, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0].boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('test1.jpg', cv2_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_np = inp[:, :, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(frame_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model inside the process\n",
    "model = YOLO('Models/model_det_test/best.engine', task='detect')\n",
    "\n",
    "model_seg = YOLO('Models/model_seg_fp32/best_yoloseg.engine', task='segment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CUDA context first\n",
    "torch.cuda.init()\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# Initialize GPU monitoring\n",
    "\n",
    "# # Initialize video capture after CUDA init\n",
    "cap = ffmpegcv.VideoCaptureNV('video.mp4', pix_fmt='rgb24')\n",
    "\n",
    "\n",
    "out = ffmpegcv.VideoWriterNV('outpy1.mp4', 'h264', 25) \n",
    "\n",
    "\n",
    "area_for_segment = [1295, 1, 2338, 2154]\n",
    "cm_per_pixel = 0.00999000999000999\n",
    "camera_id = 1\n",
    "colors = [144, 80, 70]\n",
    "\n",
    "# === Tracking state ===\n",
    "frame_count = 0\n",
    "counter = 0\n",
    "map_track_size = {}\n",
    "track_set = set()\n",
    "\n",
    "# Prepare a dictionary to store all potato data\n",
    "potato_data = {}\n",
    "\n",
    "# Performance monitoring setup\n",
    "process = psutil.Process(os.getpid())\n",
    "frame_count = 0\n",
    "last_log_time = time.time()\n",
    "start_time = last_log_time\n",
    "\n",
    "while True:\n",
    "    # Read frame\n",
    "    ret, orig_frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess\n",
    "    with torch.no_grad():\n",
    "        frame_np = orig_frame[:, :, ::-1]\n",
    "        \n",
    "        # Inference with tracking\n",
    "        results = model.track(frame_np, conf=0.6, persist=True, tracker='bytetrack_custom.yaml')\n",
    "\n",
    "        annotated_frame = frame_np.copy()\n",
    "        cv2.rectangle(annotated_frame, (area_for_segment[0], area_for_segment[1]),\n",
    "                (area_for_segment[2], area_for_segment[3]), (255, 0, 0), 2)\n",
    "\n",
    "        potato_boxes = []\n",
    "        potato_img_boxes = []\n",
    "        _, orig_h, orig_w = orig_frame.shape \n",
    "\n",
    "        resized_size = 640\n",
    "        scale_x = 1 #orig_w / resized_size\n",
    "        scale_y = 1 #orig_h / resized_size\n",
    "\n",
    "        if results and results[0].boxes is not None:\n",
    "            for box in results[0].boxes:\n",
    "                if box.id == None:\n",
    "                    continue\n",
    "                coords = box.xyxy[0].cpu().numpy().astype(int)\n",
    "                x1, y1, x2, y2 = coords\n",
    "                x1 = int(x1 * scale_x)\n",
    "                x2 = int(x2 * scale_x)\n",
    "                y1 = int(y1 * scale_y)\n",
    "                y2 = int(y2 * scale_y)\n",
    "\n",
    "\n",
    "                track_id = int(box.id.cpu().numpy()[0])\n",
    "                track_set.add(track_id)\n",
    "\n",
    "                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "\n",
    "                prev_size = map_track_size.get(track_id, (0, 0))\n",
    "                cv2.putText(annotated_frame, f\"ID: {track_id}\", (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "                cv2.putText(annotated_frame, f\"{round(prev_size[0], 2)}cm {round(prev_size[1], 2)}cm\",\n",
    "                            (x1, y1 + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "\n",
    "                if (area_for_segment[0] <= x1 <= area_for_segment[2] and\n",
    "                    area_for_segment[1] <= y1 <= area_for_segment[3] and\n",
    "                    area_for_segment[0] <= x2 <= area_for_segment[2] and\n",
    "                    area_for_segment[1] <= y2 <= area_for_segment[3] and\n",
    "                    prev_size[0] == 0 and prev_size[1] == 0):\n",
    "\n",
    "                    img_box = frame_np[y1:y2, x1:x2]\n",
    "                    potato_img_boxes.append(img_box)\n",
    "                    potato_boxes.append([x1, y1, x2, y2, track_id])\n",
    "                \n",
    "                counter += 1\n",
    "\n",
    "                data = {\n",
    "                    \"camera_id\": camera_id,\n",
    "                    \"potato_id\": track_id,  # Unique track ID\n",
    "                    \"type\": \"potato\",\n",
    "                    \"size\": f\"{round(prev_size[0], 2)}cm {round(prev_size[1], 2)}cm\",\n",
    "                    \"coordinates\": [x1, y1, x2, y2],\n",
    "                    \"frame_id\": frame_count,  # Frame ID\n",
    "                    \"passed\": \"yes\",  # Example status\n",
    "                    \"sorted\": \"no\"  # Example status\n",
    "                }\n",
    "\n",
    "                # Store the data in the dictionary using the potato_id as the key\n",
    "                potato_data[track_id] = data\n",
    "\n",
    "        # Segmentation\n",
    "        if potato_img_boxes:\n",
    "            results_seg = model_seg.predict(potato_img_boxes, imgsz=320)\n",
    "            for i, result in enumerate(results_seg):\n",
    "                x1, y1, x2, y2, track_id = potato_boxes[i]\n",
    "                prev_major, prev_minor = map_track_size.get(track_id, (0, 0))\n",
    "\n",
    "                if result.masks:\n",
    "                    for mask in result.masks.xy:\n",
    "                        abs_coords = mask + np.array([x1, y1])\n",
    "                        abs_coords = abs_coords.astype(np.int32)\n",
    "\n",
    "                        contour = np.int32([abs_coords]).reshape((-1, 1, 2))\n",
    "\n",
    "                        if len(contour) >= 5:\n",
    "                            ellipse = cv2.fitEllipse(contour)\n",
    "                            _, axes, _ = ellipse\n",
    "                            major_axis = max(axes) * cm_per_pixel\n",
    "                            minor_axis = min(axes) * cm_per_pixel\n",
    "\n",
    "                            avg_major = (major_axis + prev_major) / 2 if prev_major else major_axis\n",
    "                            avg_minor = (minor_axis + prev_minor) / 2 if prev_minor else minor_axis\n",
    "\n",
    "                            map_track_size[track_id] = (avg_major, avg_minor)\n",
    "\n",
    "                            cv2.fillPoly(annotated_frame, [contour], colors)\n",
    "        Count = 0 if len(track_set) == 0 else max(track_set)\n",
    "        cv2.putText(annotated_frame, f\"Potato Count: {Count}\",\n",
    "                    (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 2)\n",
    "        \n",
    "        out.write(annotated_frame)\n",
    "\n",
    "        potato_img_boxes = []\n",
    "        potato_boxes = []\n",
    "        img_box = None\n",
    "        annotated_frame = None\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
