{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpegcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('Models/model_det_test/best.engine', task='detect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = ffmpegcv.VideoCaptureNV('video.mp4', pix_fmt='bgr24', resize=(1920, 1080))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, frame = cap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Models/model_det_test/best.engine for TensorRT inference...\n",
      "[04/24/2025-01:04:23] [TRT] [I] Loaded engine size: 9 MiB\n",
      "[04/24/2025-01:04:23] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +181, now: CPU 0, GPU 186 (MiB)\n",
      "\n",
      "0: 384x640 17 potatos, 2.1ms\n",
      "Speed: 19.2ms preprocess, 2.1ms inference, 153.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'potato'}\n",
       " obb: None\n",
       " orig_img: array([[[ 97, 123,  52],\n",
       "         [ 97, 123,  52],\n",
       "         [ 97, 123,  52],\n",
       "         ...,\n",
       "         [122, 143,  93],\n",
       "         [122, 143,  93],\n",
       "         [122, 143,  93]],\n",
       " \n",
       "        [[ 96, 124,  52],\n",
       "         [ 96, 124,  52],\n",
       "         [ 96, 124,  52],\n",
       "         ...,\n",
       "         [121, 144,  93],\n",
       "         [121, 144,  93],\n",
       "         [121, 144,  93]],\n",
       " \n",
       "        [[ 97, 123,  51],\n",
       "         [ 97, 123,  51],\n",
       "         [ 97, 123,  51],\n",
       "         ...,\n",
       "         [122, 143,  93],\n",
       "         [122, 143,  93],\n",
       "         [122, 143,  93]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[118, 138,  66],\n",
       "         [120, 140,  68],\n",
       "         [120, 140,  68],\n",
       "         ...,\n",
       "         [135, 163,  95],\n",
       "         [135, 163,  95],\n",
       "         [135, 163,  95]],\n",
       " \n",
       "        [[116, 137,  65],\n",
       "         [117, 138,  66],\n",
       "         [117, 138,  66],\n",
       "         ...,\n",
       "         [132, 161,  94],\n",
       "         [132, 161,  94],\n",
       "         [132, 161,  94]],\n",
       " \n",
       "        [[116, 137,  65],\n",
       "         [117, 138,  66],\n",
       "         [117, 138,  66],\n",
       "         ...,\n",
       "         [132, 161,  94],\n",
       "         [132, 161,  94],\n",
       "         [132, 161,  94]]], dtype=uint8)\n",
       " orig_shape: (1080, 1920)\n",
       " path: 'image0.jpg'\n",
       " probs: None\n",
       " save_dir: '/home/paperspace/project_conveyer/runs/detect/track'\n",
       " speed: {'preprocess': 19.198250999579614, 'inference': 2.0897930003229703, 'postprocess': 153.66866700014725}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.track(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.9.0.34'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorrt.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è TensorRT requires GPU export, automatically assigning device=0\n",
      "Ultralytics 8.3.114 üöÄ Python-3.10.12 torch-2.8.0.dev20250408+cu128 CUDA:0 (NVIDIA RTX A6000, 48550MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'Models/model_det_test/best.pt' with input shape (20, 3, 640, 640) BCHW and output shape(s) (20, 5, 8400) (6.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.50...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 11.6s, saved as 'Models/model_det_test/best.onnx' (11.6 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.9.0.34...\n",
      "[04/24/2025-01:31:16] [TRT] [I] The logger passed into createInferBuilder differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "[04/24/2025-01:31:16] [TRT] [I] ----------------------------------------------------------------\n",
      "[04/24/2025-01:31:16] [TRT] [I] Input filename:   Models/model_det_test/best.onnx\n",
      "[04/24/2025-01:31:16] [TRT] [I] ONNX IR version:  0.0.9\n",
      "[04/24/2025-01:31:16] [TRT] [I] Opset version:    19\n",
      "[04/24/2025-01:31:16] [TRT] [I] Producer name:    pytorch\n",
      "[04/24/2025-01:31:16] [TRT] [I] Producer version: 2.8.0\n",
      "[04/24/2025-01:31:16] [TRT] [I] Domain:           \n",
      "[04/24/2025-01:31:16] [TRT] [I] Model version:    0\n",
      "[04/24/2025-01:31:16] [TRT] [I] Doc string:       \n",
      "[04/24/2025-01:31:16] [TRT] [I] ----------------------------------------------------------------\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(-1, 3, -1, -1) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(-1, 5, -1) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as Models/model_det_test/best.engine\n",
      "[04/24/2025-01:31:16] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[04/24/2025-01:35:17] [TRT] [I] Detected 1 inputs and 3 output network tensors.\n",
      "[04/24/2025-01:35:19] [TRT] [I] Total Host Persistent Memory: 384560 bytes\n",
      "[04/24/2025-01:35:19] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[04/24/2025-01:35:19] [TRT] [I] Max Scratch Memory: 0 bytes\n",
      "[04/24/2025-01:35:19] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 145 steps to complete.\n",
      "[04/24/2025-01:35:19] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 8.77636ms to assign 10 blocks to 145 nodes requiring 189606400 bytes.\n",
      "[04/24/2025-01:35:19] [TRT] [I] Total Activation Memory: 189605376 bytes\n",
      "[04/24/2025-01:35:19] [TRT] [I] Total Weights Memory: 6040072 bytes\n",
      "[04/24/2025-01:35:19] [TRT] [I] Engine generation completed in 243.192 seconds.\n",
      "[04/24/2025-01:35:19] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 500 MiB\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ‚úÖ 254.9s, saved as 'Models/model_det_test/best.engine' (9.6 MB)\n",
      "\n",
      "Export complete (255.1s)\n",
      "Results saved to \u001b[1m/home/paperspace/project_conveyer/Models/model_det_test\u001b[0m\n",
      "Predict:         yolo predict task=detect model=Models/model_det_test/best.engine imgsz=640 half \n",
      "Validate:        yolo val task=detect model=Models/model_det_test/best.engine imgsz=640 data=C:\\Users\\Asus Rog Strix\\Desktop\\Multimodal\\potato check.v4i.yolov8\\data.yaml half \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Models/model_det_test/best.engine'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"Models/model_det_test/best.pt\")\n",
    "model.export(\n",
    "    format=\"engine\",\n",
    "    dynamic=True,  \n",
    "    imgsz=(640, 640),\n",
    "    batch=20,  \n",
    "    half=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"Models/model_seg_fp32/best_yoloseg.pt\")\n",
    "\n",
    "engine_file = model.export(format=\"engine\", simplify=True, dynamic=True, imgsz=(320, 320), half=True, batch=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp -r /home/paperspace/project_conveyer/Datasets /home/paperspace/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpegcv\n",
    "from ultralytics import YOLO\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import psutil\n",
    "from pynvml import *\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('Models/model_det_test/best.engine', task='track')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLO()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.25M/6.25M [00:00<00:00, 331MB/s]\n"
     ]
    }
   ],
   "source": [
    "model1 = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_check_is_pytorch_model',\n",
       " '_compiled_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_new',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_reset_ckpt_args',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_smart_load',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_callback',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'benchmark',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'callbacks',\n",
       " 'cfg',\n",
       " 'children',\n",
       " 'ckpt',\n",
       " 'ckpt_path',\n",
       " 'clear_callback',\n",
       " 'compile',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'device',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'embed',\n",
       " 'eval',\n",
       " 'export',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'fuse',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'info',\n",
       " 'ipu',\n",
       " 'is_hub_model',\n",
       " 'is_triton_model',\n",
       " 'load',\n",
       " 'load_state_dict',\n",
       " 'metrics',\n",
       " 'model',\n",
       " 'model_name',\n",
       " 'modules',\n",
       " 'mtia',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'names',\n",
       " 'overrides',\n",
       " 'parameters',\n",
       " 'predict',\n",
       " 'predictor',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_load_state_dict_pre_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_post_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'reset_callbacks',\n",
       " 'reset_weights',\n",
       " 'save',\n",
       " 'session',\n",
       " 'set_extra_state',\n",
       " 'set_submodule',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'task',\n",
       " 'task_map',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'track',\n",
       " 'train',\n",
       " 'trainer',\n",
       " 'transforms',\n",
       " 'tune',\n",
       " 'type',\n",
       " 'val',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpegcv\n",
    "cap = ffmpegcv.VideoCaptureNV('video.mp4', pix_fmt='bgr24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, orig_frame = cap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2160, 3840, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "orig_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pycuda==2025.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_frame.shape\n",
    "torch.Size([3, 2160, 3840])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —É —Ç–µ–±—è –µ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ orig_frame —Å shape [3, 2160, 3840]\n",
    "# –î–æ–±–∞–≤–∏–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞\n",
    "input_tensor = orig_frame.unsqueeze(0)  # -> [1, 3, 2160, 3840]\n",
    "\n",
    "# –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ —Ç–µ–Ω–∑–æ—Ä –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —Ç–æ–º –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ, —á—Ç–æ –∏ –º–æ–¥–µ–ª—å\n",
    "input_tensor = input_tensor.float() / 255.0  # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏\n",
    "with torch.no_grad():\n",
    "    results = model(input_tensor, imgsz=640)\n",
    "\n",
    "# –ù–∞–ø—Ä–∏–º–µ—Ä, –≤—ã–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0].boxes[0].xyxy[0].cpu().numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = ffmpegcv.toCUDA(ffmpegcv.VideoCaptureNV('video.mp4', pix_fmt='nv12'), tensor_format='chw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = orig_frame.permute(1, 2, 0).cpu().numpy()\n",
    "cv2_image = cv2.cvtColor(inp, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = cv2.imread('test1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(n, conf=0.5, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0].boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('test1.jpg', cv2_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_np = inp[:, :, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(frame_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model inside the process\n",
    "model = YOLO('Models/model_det_test/best.engine', task='detect')\n",
    "\n",
    "model_seg = YOLO('Models/model_seg_fp32/best_yoloseg.engine', task='segment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CUDA context first\n",
    "torch.cuda.init()\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# Initialize GPU monitoring\n",
    "\n",
    "# # Initialize video capture after CUDA init\n",
    "cap = ffmpegcv.VideoCaptureNV('video.mp4', pix_fmt='rgb24')\n",
    "\n",
    "\n",
    "out = ffmpegcv.VideoWriterNV('outpy1.mp4', 'h264', 25) \n",
    "\n",
    "\n",
    "area_for_segment = [1295, 1, 2338, 2154]\n",
    "cm_per_pixel = 0.00999000999000999\n",
    "camera_id = 1\n",
    "colors = [144, 80, 70]\n",
    "\n",
    "# === Tracking state ===\n",
    "frame_count = 0\n",
    "counter = 0\n",
    "map_track_size = {}\n",
    "track_set = set()\n",
    "\n",
    "# Prepare a dictionary to store all potato data\n",
    "potato_data = {}\n",
    "\n",
    "# Performance monitoring setup\n",
    "process = psutil.Process(os.getpid())\n",
    "frame_count = 0\n",
    "last_log_time = time.time()\n",
    "start_time = last_log_time\n",
    "\n",
    "while True:\n",
    "    # Read frame\n",
    "    ret, orig_frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess\n",
    "    with torch.no_grad():\n",
    "        frame_np = orig_frame[:, :, ::-1]\n",
    "        \n",
    "        # Inference with tracking\n",
    "        results = model.track(frame_np, conf=0.6, persist=True, tracker='bytetrack_custom.yaml')\n",
    "\n",
    "        annotated_frame = frame_np.copy()\n",
    "        cv2.rectangle(annotated_frame, (area_for_segment[0], area_for_segment[1]),\n",
    "                (area_for_segment[2], area_for_segment[3]), (255, 0, 0), 2)\n",
    "\n",
    "        potato_boxes = []\n",
    "        potato_img_boxes = []\n",
    "        _, orig_h, orig_w = orig_frame.shape \n",
    "\n",
    "        resized_size = 640\n",
    "        scale_x = 1 #orig_w / resized_size\n",
    "        scale_y = 1 #orig_h / resized_size\n",
    "\n",
    "        if results and results[0].boxes is not None:\n",
    "            for box in results[0].boxes:\n",
    "                if box.id == None:\n",
    "                    continue\n",
    "                coords = box.xyxy[0].cpu().numpy().astype(int)\n",
    "                x1, y1, x2, y2 = coords\n",
    "                x1 = int(x1 * scale_x)\n",
    "                x2 = int(x2 * scale_x)\n",
    "                y1 = int(y1 * scale_y)\n",
    "                y2 = int(y2 * scale_y)\n",
    "\n",
    "\n",
    "                track_id = int(box.id.cpu().numpy()[0])\n",
    "                track_set.add(track_id)\n",
    "\n",
    "                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "\n",
    "                prev_size = map_track_size.get(track_id, (0, 0))\n",
    "                cv2.putText(annotated_frame, f\"ID: {track_id}\", (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "                cv2.putText(annotated_frame, f\"{round(prev_size[0], 2)}cm {round(prev_size[1], 2)}cm\",\n",
    "                            (x1, y1 + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "\n",
    "                if (area_for_segment[0] <= x1 <= area_for_segment[2] and\n",
    "                    area_for_segment[1] <= y1 <= area_for_segment[3] and\n",
    "                    area_for_segment[0] <= x2 <= area_for_segment[2] and\n",
    "                    area_for_segment[1] <= y2 <= area_for_segment[3] and\n",
    "                    prev_size[0] == 0 and prev_size[1] == 0):\n",
    "\n",
    "                    img_box = frame_np[y1:y2, x1:x2]\n",
    "                    potato_img_boxes.append(img_box)\n",
    "                    potato_boxes.append([x1, y1, x2, y2, track_id])\n",
    "                \n",
    "                counter += 1\n",
    "\n",
    "                data = {\n",
    "                    \"camera_id\": camera_id,\n",
    "                    \"potato_id\": track_id,  # Unique track ID\n",
    "                    \"type\": \"potato\",\n",
    "                    \"size\": f\"{round(prev_size[0], 2)}cm {round(prev_size[1], 2)}cm\",\n",
    "                    \"coordinates\": [x1, y1, x2, y2],\n",
    "                    \"frame_id\": frame_count,  # Frame ID\n",
    "                    \"passed\": \"yes\",  # Example status\n",
    "                    \"sorted\": \"no\"  # Example status\n",
    "                }\n",
    "\n",
    "                # Store the data in the dictionary using the potato_id as the key\n",
    "                potato_data[track_id] = data\n",
    "\n",
    "        # Segmentation\n",
    "        if potato_img_boxes:\n",
    "            results_seg = model_seg.predict(potato_img_boxes, imgsz=320)\n",
    "            for i, result in enumerate(results_seg):\n",
    "                x1, y1, x2, y2, track_id = potato_boxes[i]\n",
    "                prev_major, prev_minor = map_track_size.get(track_id, (0, 0))\n",
    "\n",
    "                if result.masks:\n",
    "                    for mask in result.masks.xy:\n",
    "                        abs_coords = mask + np.array([x1, y1])\n",
    "                        abs_coords = abs_coords.astype(np.int32)\n",
    "\n",
    "                        contour = np.int32([abs_coords]).reshape((-1, 1, 2))\n",
    "\n",
    "                        if len(contour) >= 5:\n",
    "                            ellipse = cv2.fitEllipse(contour)\n",
    "                            _, axes, _ = ellipse\n",
    "                            major_axis = max(axes) * cm_per_pixel\n",
    "                            minor_axis = min(axes) * cm_per_pixel\n",
    "\n",
    "                            avg_major = (major_axis + prev_major) / 2 if prev_major else major_axis\n",
    "                            avg_minor = (minor_axis + prev_minor) / 2 if prev_minor else minor_axis\n",
    "\n",
    "                            map_track_size[track_id] = (avg_major, avg_minor)\n",
    "\n",
    "                            cv2.fillPoly(annotated_frame, [contour], colors)\n",
    "        Count = 0 if len(track_set) == 0 else max(track_set)\n",
    "        cv2.putText(annotated_frame, f\"Potato Count: {Count}\",\n",
    "                    (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 2)\n",
    "        \n",
    "        out.write(annotated_frame)\n",
    "\n",
    "        potato_img_boxes = []\n",
    "        potato_boxes = []\n",
    "        img_box = None\n",
    "        annotated_frame = None\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
