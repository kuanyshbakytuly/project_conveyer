{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"Models/model_det_test/best.pt\")\n",
    "\n",
    "engine_file = model.export(format=\"engine\", simplify=True, dynamic=True, imgsz=(640, 640), half=True, batch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpegcv\n",
    "from ultralytics import YOLO\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import psutil\n",
    "from pynvml import *\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('Models/model_det_test/best.engine', task='detect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = ffmpegcv.toCUDA(ffmpegcv.VideoCaptureNV('video.mp4', pix_fmt='nv12'), tensor_format='chw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, orig_frame = cap.read_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2160, 3840])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_frame.shape\n",
    "torch.Size([3, 2160, 3840])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "WARNING ⚠️ torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 2160, 3840) is incompatible.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Запуск модели\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 12\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Например, вывести результаты:\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "File \u001b[0;32m~/project_conveyer/.venv/lib/python3.10/site-packages/ultralytics/engine/model.py:182\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    155\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    156\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    158\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project_conveyer/.venv/lib/python3.10/site-packages/ultralytics/engine/model.py:550\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project_conveyer/.venv/lib/python3.10/site-packages/ultralytics/engine/predictor.py:216\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project_conveyer/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/project_conveyer/.venv/lib/python3.10/site-packages/ultralytics/engine/predictor.py:297\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_model(model)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:  \u001b[38;5;66;03m# for thread-safe inference\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;66;03m# Setup source every time predict is called\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# Check if save_dir/ label file exists\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_txt:\n",
      "File \u001b[0;32m~/project_conveyer/.venv/lib/python3.10/site-packages/ultralytics/engine/predictor.py:257\u001b[0m, in \u001b[0;36mBasePredictor.setup_source\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgsz \u001b[38;5;241m=\u001b[39m check_imgsz(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mimgsz, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstride, min_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# check image size\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    256\u001b[0m )\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_inference_source\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvid_stride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvid_stride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39msource_type\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mstream\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mscreenshot\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# many images\u001b[39;00m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo_flag\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;28;01mFalse\u001b[39;00m]))\n\u001b[1;32m    269\u001b[0m ):  \u001b[38;5;66;03m# videos\u001b[39;00m\n",
      "File \u001b[0;32m~/project_conveyer/.venv/lib/python3.10/site-packages/ultralytics/data/build.py:243\u001b[0m, in \u001b[0;36mload_inference_source\u001b[0;34m(source, batch, vid_stride, buffer)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Dataloader\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor:\n\u001b[0;32m--> 243\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mLoadTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m in_memory:\n\u001b[1;32m    245\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m source\n",
      "File \u001b[0;32m~/project_conveyer/.venv/lib/python3.10/site-packages/ultralytics/data/loaders.py:543\u001b[0m, in \u001b[0;36mLoadTensor.__init__\u001b[0;34m(self, im0)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, im0) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    542\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize LoadTensor object for processing torch.Tensor image data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_single_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim0\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/project_conveyer/.venv/lib/python3.10/site-packages/ultralytics/data/loaders.py:561\u001b[0m, in \u001b[0;36mLoadTensor._single_check\u001b[0;34m(im, stride)\u001b[0m\n\u001b[1;32m    559\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m%\u001b[39m stride \u001b[38;5;129;01mor\u001b[39;00m im\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m%\u001b[39m stride:\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(s)\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(im\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39meps:  \u001b[38;5;66;03m# torch.float32 eps is 1.2e-07\u001b[39;00m\n\u001b[1;32m    563\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    564\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mim\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    565\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDividing input by 255.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: WARNING ⚠️ torch.Tensor inputs should be BCHW i.e. shape(1, 3, 640, 640) divisible by stride 32. Input shape(1, 3, 2160, 3840) is incompatible."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Предположим, у тебя есть изображение orig_frame с shape [3, 2160, 3840]\n",
    "# Добавим размер батча\n",
    "input_tensor = orig_frame.unsqueeze(0)  # -> [1, 3, 2160, 3840]\n",
    "\n",
    "# Убедимся, что тензор находится на том же устройстве, что и модель\n",
    "input_tensor = input_tensor.float() / 255.0  # нормализация\n",
    "\n",
    "# Запуск модели\n",
    "with torch.no_grad():\n",
    "    results = model(input_tensor, imgsz=640)\n",
    "\n",
    "# Например, вывести результаты:\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0, 1700,  923])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "res[0].boxes[0].xyxy[0].cpu().numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = ffmpegcv.toCUDA(ffmpegcv.VideoCaptureNV('video.mp4', pix_fmt='nv12'), tensor_format='chw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = orig_frame.permute(1, 2, 0).cpu().numpy()\n",
    "cv2_image = cv2.cvtColor(inp, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2160, 3840, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = cv2.imread('test1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[04/16/2025-21:37:33] [TRT] [E] IExecutionContext::executeV2: Error Code 1: Cuda Runtime (invalid resource handle)\n",
      "0: 384x640 300 potatos, 0.3ms\n",
      "Speed: 2.7ms preprocess, 0.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(n, conf=0.5, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "cls: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       device='cuda:0')\n",
       "conf: tensor([3.4028e+38, 3.3916e+38, 1.9960e+38, 1.9141e+38, 1.5685e+38, 8.2488e+37, 5.6496e+37, 4.2535e+37, 1.4123e+37, 1.3665e+37, 5.2364e+36, 5.1554e+36, 5.0288e+36, 3.8634e+36, 3.8632e+36, 3.8439e+36, 3.8423e+36, 3.8215e+36, 3.5585e+36, 3.5518e+36, 3.5134e+36, 3.4962e+36, 3.4749e+36, 3.4743e+36, 3.4549e+36, 3.4548e+36,\n",
       "        3.4547e+36, 3.4537e+36, 3.4537e+36, 3.4534e+36, 3.4525e+36, 3.4525e+36, 3.4521e+36, 3.4521e+36, 3.4521e+36, 3.4127e+36, 3.4113e+36, 3.4101e+36, 3.3291e+36, 3.3291e+36, 3.3290e+36, 3.3290e+36, 3.3289e+36, 3.3287e+36, 3.3287e+36, 3.3237e+36, 3.3234e+36, 3.3234e+36, 3.3231e+36, 3.3231e+36, 3.1454e+36, 3.0324e+36,\n",
       "        1.3500e+36, 1.3056e+36, 1.2619e+36, 8.7310e+35, 3.3489e+35, 3.2728e+35, 2.1033e+35, 1.6607e+35, 1.2656e+35, 8.7187e+34, 8.1139e+34, 8.0858e+34, 7.9227e+34, 7.0096e+34, 6.4648e+34, 5.5168e+34, 4.2695e+34, 2.2718e+34, 1.9717e+34, 1.8292e+34, 1.5254e+34, 1.5172e+34, 1.5092e+34, 1.5091e+34, 1.5091e+34, 1.5090e+34,\n",
       "        1.5029e+34, 1.5012e+34, 1.4766e+34, 1.4766e+34, 1.4685e+34, 1.3715e+34, 1.3714e+34, 1.3711e+34, 1.3062e+34, 1.3005e+34, 1.3004e+34, 1.3004e+34, 1.3004e+34, 1.3004e+34, 1.3004e+34, 1.3004e+34, 1.3004e+34, 1.3004e+34, 1.3003e+34, 1.3003e+34, 1.2983e+34, 1.2287e+34, 1.1845e+34, 1.1369e+34, 1.0563e+34, 1.0552e+34,\n",
       "        1.0395e+34, 1.0388e+34, 1.0385e+34, 4.8565e+33, 4.7134e+33, 4.5271e+33, 3.2452e+33, 3.1657e+33, 3.1480e+33, 2.6056e+33, 2.5860e+33, 1.2498e+33, 1.2496e+33, 1.2443e+33, 1.2291e+33, 1.2087e+33, 1.1834e+33, 8.4680e+32, 7.2516e+32, 6.5950e+32, 3.1466e+32, 3.1456e+32, 3.1078e+32, 3.0948e+32, 3.0948e+32, 2.8826e+32,\n",
       "        2.8699e+32, 2.6894e+32, 1.6284e+32, 1.6226e+32, 7.8597e+31, 7.6740e+31, 7.6277e+31, 7.5556e+31, 7.5546e+31, 5.8946e+31, 5.7997e+31, 5.7783e+31, 5.1748e+31, 5.1748e+31, 5.1746e+31, 5.1746e+31, 5.1746e+31, 5.1745e+31, 5.1745e+31, 5.1742e+31, 5.1742e+31, 5.1724e+31, 4.8111e+31, 4.7042e+31, 4.5002e+31, 4.0572e+31,\n",
       "        3.0429e+31, 1.9288e+31, 1.9196e+31, 1.8891e+31, 1.8888e+31, 1.8417e+31, 1.8254e+31, 1.3822e+31, 1.2763e+31, 1.2709e+31, 1.1884e+31, 1.0803e+31, 1.0803e+31, 1.0803e+31, 1.0803e+31, 1.0802e+31, 1.0802e+31, 1.0801e+31, 1.0786e+31, 1.0780e+31, 1.0778e+31, 1.0777e+31, 1.0775e+31, 1.0177e+31, 5.5460e+30, 5.3611e+30,\n",
       "        5.3479e+30, 4.8403e+30, 4.8391e+30, 4.7985e+30, 4.7616e+30, 4.5630e+30, 4.5443e+30, 4.5426e+30, 4.5360e+30, 4.4647e+30, 4.4601e+30, 3.9827e+30, 3.9813e+30, 3.9812e+30, 3.9482e+30, 3.9423e+30, 3.9419e+30, 3.9416e+30, 3.9416e+30, 3.9416e+30, 3.9087e+30, 3.9064e+30, 3.9064e+30, 3.9039e+30, 3.9033e+30, 3.9032e+30,\n",
       "        3.9023e+30, 3.9020e+30, 3.8691e+30, 3.8691e+30, 3.8689e+30, 3.8689e+30, 3.8689e+30, 3.8673e+30, 3.8650e+30, 3.8630e+30, 3.6484e+30, 3.6471e+30, 3.6264e+30, 3.6247e+30, 3.5917e+30, 3.4872e+30, 3.4692e+30, 3.3936e+30, 3.3475e+30, 3.3276e+30, 3.2935e+30, 3.2925e+30, 3.2922e+30, 3.2881e+30, 3.2354e+30, 3.2354e+30,\n",
       "        3.2353e+30, 3.2348e+30, 3.2343e+30, 3.2343e+30, 3.2341e+30, 3.2341e+30, 3.2340e+30, 3.2340e+30, 3.2340e+30, 3.2335e+30, 3.2335e+30, 3.2304e+30, 3.2292e+30, 3.1894e+30, 3.1760e+30, 3.1760e+30, 3.1759e+30, 3.1759e+30, 3.1759e+30, 3.1759e+30, 3.1759e+30, 3.1759e+30, 3.1759e+30, 3.1759e+30, 3.1729e+30, 3.1691e+30,\n",
       "        3.1691e+30, 3.1185e+30, 3.0311e+30, 2.9751e+30, 2.8368e+30, 2.6803e+30, 2.6795e+30, 2.6778e+30, 2.5365e+30, 1.1941e+30, 6.6926e+29, 5.6471e+29, 4.9270e+29, 4.1595e+29, 4.0129e+29, 3.5184e+29, 3.2475e+29, 3.0964e+29, 3.0759e+29, 3.0718e+29, 3.0459e+29, 3.0012e+29, 2.9971e+29, 2.8149e+29, 2.7902e+29, 2.5876e+29,\n",
       "        2.4016e+29, 2.1912e+29, 2.1747e+29, 2.1418e+29, 2.1171e+29, 2.0678e+29, 2.0614e+29, 1.9825e+29, 1.9819e+29, 1.9346e+29, 1.9076e+29, 1.8955e+29, 1.8704e+29, 1.8455e+29], device='cuda:0')\n",
       "data: tensor([[0.0000e+00, 0.0000e+00, 3.8400e+03, 2.1600e+03, 3.4028e+38, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 3.8400e+03, 0.0000e+00, 3.3916e+38, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9960e+38, 0.0000e+00],\n",
       "        ...,\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8955e+29, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8704e+29, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8455e+29, 0.0000e+00]], device='cuda:0')\n",
       "id: None\n",
       "is_track: False\n",
       "orig_shape: (2160, 3840)\n",
       "shape: torch.Size([300, 6])\n",
       "xywh: tensor([[1920., 1080., 3840., 2160.],\n",
       "        [1920.,    0., 3840.,    0.],\n",
       "        [   0.,    0.,    0.,    0.],\n",
       "        ...,\n",
       "        [   0.,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0.,    0.]], device='cuda:0')\n",
       "xywhn: tensor([[0.5000, 0.5000, 1.0000, 1.0000],\n",
       "        [0.5000, 0.0000, 1.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0')\n",
       "xyxy: tensor([[   0.,    0., 3840., 2160.],\n",
       "        [   0.,    0., 3840.,    0.],\n",
       "        [   0.,    0.,    0.,    0.],\n",
       "        ...,\n",
       "        [   0.,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0.,    0.]], device='cuda:0')\n",
       "xyxyn: tensor([[0., 0., 1., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0].boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project_conveyer/.venv/lib/python3.10/site-packages/ultralytics/engine/results.py:554\u001b[0m, in \u001b[0;36mResults.plot\u001b[0;34m(self, conf, line_width, font_size, font, pil, img, im_gpu, kpt_radius, kpt_line, labels, boxes, masks, probs, show, save, filename, color_mode, txt_color)\u001b[0m\n\u001b[1;32m    552\u001b[0m         label \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_conf\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conf \u001b[38;5;28;01melse\u001b[39;00m name) \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    553\u001b[0m         box \u001b[38;5;241m=\u001b[39m d\u001b[38;5;241m.\u001b[39mxyxyxyxy\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze() \u001b[38;5;28;01mif\u001b[39;00m is_obb \u001b[38;5;28;01melse\u001b[39;00m d\u001b[38;5;241m.\u001b[39mxyxy\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m--> 554\u001b[0m         \u001b[43mannotator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbox_label\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m                \u001b[49m\u001b[43mc\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    560\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\n\u001b[1;32m    561\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    562\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minstance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    564\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrotated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_obb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;66;03m# Plot Classify results\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pred_probs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m show_probs:\n",
      "File \u001b[0;32m~/project_conveyer/.venv/lib/python3.10/site-packages/ultralytics/utils/plotting.py:319\u001b[0m, in \u001b[0;36mAnnotator.box_label\u001b[0;34m(self, box, label, color, txt_color, rotated)\u001b[0m\n\u001b[1;32m    317\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mpolylines(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim, [np\u001b[38;5;241m.\u001b[39masarray(box, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)], \u001b[38;5;28;01mTrue\u001b[39;00m, color, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlw)  \u001b[38;5;66;03m# cv2 requires nparray box\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     p1, p2 \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbox\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mint\u001b[39m(box[\u001b[38;5;241m1\u001b[39m])), (\u001b[38;5;28mint\u001b[39m(box[\u001b[38;5;241m2\u001b[39m]), \u001b[38;5;28mint\u001b[39m(box[\u001b[38;5;241m3\u001b[39m]))\n\u001b[1;32m    320\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mrectangle(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim, p1, p2, color, thickness\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlw, lineType\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mLINE_AA)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label:\n",
      "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "res[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@634.524] global loadsave.cpp:848 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('test1.jpg', cv2_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2160, 3840)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2160, 3840])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_np = inp[:, :, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) /io/opencv/modules/imgproc/src/resize.cpp:3845: error: (-215:Assertion failed) !dsize.empty() in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_np\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project_conveyer/.venv/lib/python3.10/site-packages/ultralytics/engine/model.py:182\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    155\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    156\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    158\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project_conveyer/.venv/lib/python3.10/site-packages/ultralytics/engine/model.py:550\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project_conveyer/.venv/lib/python3.10/site-packages/ultralytics/engine/predictor.py:216\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project_conveyer/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/project_conveyer/.venv/lib/python3.10/site-packages/ultralytics/engine/predictor.py:321\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# Preprocess\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 321\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim0s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[0;32m~/project_conveyer/.venv/lib/python3.10/site-packages/ultralytics/engine/predictor.py:153\u001b[0m, in \u001b[0;36mBasePredictor.preprocess\u001b[0;34m(self, im)\u001b[0m\n\u001b[1;32m    151\u001b[0m not_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(im, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_tensor:\n\u001b[0;32m--> 153\u001b[0m     im \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    154\u001b[0m     im \u001b[38;5;241m=\u001b[39m im[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# BGR to RGB, BHWC to BCHW, (n, 3, h, w)\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     im \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(im)  \u001b[38;5;66;03m# contiguous\u001b[39;00m\n",
      "File \u001b[0;32m~/project_conveyer/.venv/lib/python3.10/site-packages/ultralytics/engine/predictor.py:191\u001b[0m, in \u001b[0;36mBasePredictor.pre_transform\u001b[0;34m(self, im)\u001b[0m\n\u001b[1;32m    183\u001b[0m same_shapes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m({x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m im}) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    184\u001b[0m letterbox \u001b[38;5;241m=\u001b[39m LetterBox(\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgsz,\n\u001b[1;32m    186\u001b[0m     auto\u001b[38;5;241m=\u001b[39msame_shapes\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m     stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    190\u001b[0m )\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [letterbox(image\u001b[38;5;241m=\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m im]\n",
      "File \u001b[0;32m~/project_conveyer/.venv/lib/python3.10/site-packages/ultralytics/engine/predictor.py:191\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    183\u001b[0m same_shapes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m({x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m im}) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    184\u001b[0m letterbox \u001b[38;5;241m=\u001b[39m LetterBox(\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgsz,\n\u001b[1;32m    186\u001b[0m     auto\u001b[38;5;241m=\u001b[39msame_shapes\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m     stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    190\u001b[0m )\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mletterbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m im]\n",
      "File \u001b[0;32m~/project_conveyer/.venv/lib/python3.10/site-packages/ultralytics/data/augment.py:1589\u001b[0m, in \u001b[0;36mLetterBox.__call__\u001b[0;34m(self, labels, image)\u001b[0m\n\u001b[1;32m   1586\u001b[0m     dh \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m new_unpad:  \u001b[38;5;66;03m# resize\u001b[39;00m\n\u001b[0;32m-> 1589\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_unpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINTER_LINEAR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1590\u001b[0m top, bottom \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(dh \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.1\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenter \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(dh \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m))\n\u001b[1;32m   1591\u001b[0m left, right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(dw \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.1\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenter \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m(dw \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m))\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.11.0) /io/opencv/modules/imgproc/src/resize.cpp:3845: error: (-215:Assertion failed) !dsize.empty() in function 'resize'\n"
     ]
    }
   ],
   "source": [
    "model(frame_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model inside the process\n",
    "model = YOLO('Models/model_det_test/best.engine', task='detect')\n",
    "\n",
    "model_seg = YOLO('Models/model_seg_fp32/best_yoloseg.engine', task='segment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CUDA context first\n",
    "torch.cuda.init()\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# Initialize GPU monitoring\n",
    "\n",
    "# # Initialize video capture after CUDA init\n",
    "cap = ffmpegcv.VideoCaptureNV('video.mp4', pix_fmt='rgb24')\n",
    "\n",
    "\n",
    "out = ffmpegcv.VideoWriterNV('outpy1.mp4', 'h264', 25) \n",
    "\n",
    "\n",
    "area_for_segment = [1295, 1, 2338, 2154]\n",
    "cm_per_pixel = 0.00999000999000999\n",
    "camera_id = 1\n",
    "colors = [144, 80, 70]\n",
    "\n",
    "# === Tracking state ===\n",
    "frame_count = 0\n",
    "counter = 0\n",
    "map_track_size = {}\n",
    "track_set = set()\n",
    "\n",
    "# Prepare a dictionary to store all potato data\n",
    "potato_data = {}\n",
    "\n",
    "# Performance monitoring setup\n",
    "process = psutil.Process(os.getpid())\n",
    "frame_count = 0\n",
    "last_log_time = time.time()\n",
    "start_time = last_log_time\n",
    "\n",
    "while True:\n",
    "    # Read frame\n",
    "    ret, orig_frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess\n",
    "    with torch.no_grad():\n",
    "        frame_np = orig_frame[:, :, ::-1]\n",
    "        \n",
    "        # Inference with tracking\n",
    "        results = model.track(frame_np, conf=0.6, persist=True, tracker='bytetrack_custom.yaml')\n",
    "\n",
    "        annotated_frame = frame_np.copy()\n",
    "        cv2.rectangle(annotated_frame, (area_for_segment[0], area_for_segment[1]),\n",
    "                (area_for_segment[2], area_for_segment[3]), (255, 0, 0), 2)\n",
    "\n",
    "        potato_boxes = []\n",
    "        potato_img_boxes = []\n",
    "        _, orig_h, orig_w = orig_frame.shape \n",
    "\n",
    "        resized_size = 640\n",
    "        scale_x = 1 #orig_w / resized_size\n",
    "        scale_y = 1 #orig_h / resized_size\n",
    "\n",
    "        if results and results[0].boxes is not None:\n",
    "            for box in results[0].boxes:\n",
    "                if box.id == None:\n",
    "                    continue\n",
    "                coords = box.xyxy[0].cpu().numpy().astype(int)\n",
    "                x1, y1, x2, y2 = coords\n",
    "                x1 = int(x1 * scale_x)\n",
    "                x2 = int(x2 * scale_x)\n",
    "                y1 = int(y1 * scale_y)\n",
    "                y2 = int(y2 * scale_y)\n",
    "\n",
    "\n",
    "                track_id = int(box.id.cpu().numpy()[0])\n",
    "                track_set.add(track_id)\n",
    "\n",
    "                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "\n",
    "                prev_size = map_track_size.get(track_id, (0, 0))\n",
    "                cv2.putText(annotated_frame, f\"ID: {track_id}\", (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "                cv2.putText(annotated_frame, f\"{round(prev_size[0], 2)}cm {round(prev_size[1], 2)}cm\",\n",
    "                            (x1, y1 + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "\n",
    "                if (area_for_segment[0] <= x1 <= area_for_segment[2] and\n",
    "                    area_for_segment[1] <= y1 <= area_for_segment[3] and\n",
    "                    area_for_segment[0] <= x2 <= area_for_segment[2] and\n",
    "                    area_for_segment[1] <= y2 <= area_for_segment[3] and\n",
    "                    prev_size[0] == 0 and prev_size[1] == 0):\n",
    "\n",
    "                    img_box = frame_np[y1:y2, x1:x2]\n",
    "                    potato_img_boxes.append(img_box)\n",
    "                    potato_boxes.append([x1, y1, x2, y2, track_id])\n",
    "                \n",
    "                counter += 1\n",
    "\n",
    "                data = {\n",
    "                    \"camera_id\": camera_id,\n",
    "                    \"potato_id\": track_id,  # Unique track ID\n",
    "                    \"type\": \"potato\",\n",
    "                    \"size\": f\"{round(prev_size[0], 2)}cm {round(prev_size[1], 2)}cm\",\n",
    "                    \"coordinates\": [x1, y1, x2, y2],\n",
    "                    \"frame_id\": frame_count,  # Frame ID\n",
    "                    \"passed\": \"yes\",  # Example status\n",
    "                    \"sorted\": \"no\"  # Example status\n",
    "                }\n",
    "\n",
    "                # Store the data in the dictionary using the potato_id as the key\n",
    "                potato_data[track_id] = data\n",
    "\n",
    "        # Segmentation\n",
    "        if potato_img_boxes:\n",
    "            results_seg = model_seg.predict(potato_img_boxes, imgsz=320)\n",
    "            for i, result in enumerate(results_seg):\n",
    "                x1, y1, x2, y2, track_id = potato_boxes[i]\n",
    "                prev_major, prev_minor = map_track_size.get(track_id, (0, 0))\n",
    "\n",
    "                if result.masks:\n",
    "                    for mask in result.masks.xy:\n",
    "                        abs_coords = mask + np.array([x1, y1])\n",
    "                        abs_coords = abs_coords.astype(np.int32)\n",
    "\n",
    "                        contour = np.int32([abs_coords]).reshape((-1, 1, 2))\n",
    "\n",
    "                        if len(contour) >= 5:\n",
    "                            ellipse = cv2.fitEllipse(contour)\n",
    "                            _, axes, _ = ellipse\n",
    "                            major_axis = max(axes) * cm_per_pixel\n",
    "                            minor_axis = min(axes) * cm_per_pixel\n",
    "\n",
    "                            avg_major = (major_axis + prev_major) / 2 if prev_major else major_axis\n",
    "                            avg_minor = (minor_axis + prev_minor) / 2 if prev_minor else minor_axis\n",
    "\n",
    "                            map_track_size[track_id] = (avg_major, avg_minor)\n",
    "\n",
    "                            cv2.fillPoly(annotated_frame, [contour], colors)\n",
    "        Count = 0 if len(track_set) == 0 else max(track_set)\n",
    "        cv2.putText(annotated_frame, f\"Potato Count: {Count}\",\n",
    "                    (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 2)\n",
    "        \n",
    "        out.write(annotated_frame)\n",
    "\n",
    "        potato_img_boxes = []\n",
    "        potato_boxes = []\n",
    "        img_box = None\n",
    "        annotated_frame = None\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
